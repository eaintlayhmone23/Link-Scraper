{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"fc8E8O7meklt"},"outputs":[],"source":["# !pip install selenium webdriver-manager\n","# !apt-get update # to update ubuntu to correctly run apt install\n","# !apt install chromium-chromedriver\n","# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"NGnqlJxPdVgx"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from webdriver_manager.chrome import ChromeDriverManager\n","import os\n","import requests\n","from tqdm import tqdm\n","import numpy as np"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"XEVI7xv0ePEw"},"outputs":[],"source":["def driver_setup():\n","    options = webdriver.ChromeOptions()\n","    options.add_argument('--headless')\n","    options.add_argument('--no-sandbox')\n","    options.add_argument('--disable-dev-shm-usage')\n","    # driver = webdriver.Chrome('chromedriver', options=options)\n","    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n","    return driver\n","\n","def get_soup(URL, driver):\n","    r = requests.get(URL)\n","    if r:\n","        driver.get(URL)\n","        htmlContent = driver.page_source\n","        soup =  BeautifulSoup(htmlContent, 'html.parser')\n","        return soup\n","    else:\n","        # when return HTML error such as 404\n","        print(\"Error Fetching Soup, Scraping Ended\")\n","        return None\n","\n","def get_link_from_soup(soup):\n","    link_list = set()\n","    try:\n","        wanted_div = soup.findAll(\"div\", {\"id\":\"list\",\n","                                         \"class\":\"list ect-entry-card front-page-type-index\"})[0]\n","    except IndexError:\n","        print(\"HTML element not found, returning empty set\")\n","        return set()\n","    for div in wanted_div.findAll(\"a\", {\"class\":\"column_indexItem_thumb\"}):\n","        wanted_article = div[\"href\"]\n","        link_list.add(wanted_article)\n","    return link_list\n","\n","def crawler(chunk, driver):\n","  links_in_chunk = set()\n","  for i in tqdm(chunk):\n","    URL = f\"https://www.goo-net.com/magazine/new/page/{i}/\"\n","    soup = get_soup(URL, driver)\n","    if soup:\n","        current_links = get_link_from_soup(soup)\n","        if len(current_links) > 0: # continue until no tags are found \n","            links_in_chunk.update(current_links)\n","        else: # if tags we want are not found\n","            return links_in_chunk\n","    else: # if encountered HTML error such as 404\n","        return links_in_chunk\n","  return links_in_chunk\n","\n","def save_link_as_txt(link_list):\n","    file_path = \"links/goo-net.txt\"\n","    # if file exist, get its content, merge to existing set and then overwrite\n","    if os.path.isfile(file_path):\n","        with open(file_path, 'r+', encoding=\"utf-8\") as file:\n","            # get existing links in file\n","            old_links = set(file.readlines())\n","            # remove duplicates \n","            link_list = link_list - old_links\n","            file.seek(0)\n","            for i in link_list:\n","                file.write(i+\"\\n\")\n","            file.truncate()\n","    else: # if not, make new file\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","            for i in link_list:\n","                file.write(i+\"\\n\")\n","    print(f\"Links Saved in {file_path}\")\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4510339,"status":"ok","timestamp":1665727790013,"user":{"displayName":"Aung Seth Pai","userId":"16615101138676205606"},"user_tz":-390},"id":"o6NIcG0UipeK","outputId":"06cdb3d8-7266-4dd5-e353-dd4586e778b8"},"outputs":[],"source":["from concurrent.futures import ThreadPoolExecutor\n","\n","thread = 8\n","drivers = [driver_setup() for _ in range(thread)]\n","chunks = np.array_split(np.arange(1,1761), thread)\n","all_links = set()\n","\n","with ThreadPoolExecutor(max_workers=thread) as executor:\n","    bucket = executor.map(crawler, chunks, drivers)\n","\n","[driver.quit() for driver in drivers]\n","\n","for i in bucket:\n","  all_links.update(i)\n","\n","save_link_as_txt(all_links)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM4Y/VhIQRYzE0Kxzw91GaT","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 ('proj0': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"2f10e409dbcda6cdc0ba569de5a5e5f90bfa18acc22334e78448605fa53cf624"}}},"nbformat":4,"nbformat_minor":0}
